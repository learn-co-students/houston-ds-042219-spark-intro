{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark: Getting Started\n",
    "\n",
    "## Options:\n",
    "- Install `Spark` and `Pyspark` on your machine directly\n",
    " - Instructions below for mac\n",
    " - [Instructions for Windows](https://medium.com/@naomi.fridman/install-pyspark-to-run-on-jupyter-notebook-on-windows-4ec2009de21f)\n",
    "- Use the [learn.co](https://github.com/learn-co-students/dsc-spark-docker-installation-data-science) instructions to use a Docker container with `Spark` and `Pyspark` running in a container. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 (For Macs) : Prerequisites & Installation\n",
    "\n",
    " * These instructions require a Mac with [Anaconda3](https://anaconda.com/) and [Homebrew](https://brew.sh/) installed.\n",
    " * Useful for small data only. For larger data, try [Databricks](https://databricks.com/).\n",
    "\n",
    "Run these commands in your terminal (just once).\n",
    "\n",
    "```bash\n",
    "# Make Homebrew aware of old versions of casks\n",
    "brew tap caskroom/versions\n",
    "\n",
    "# Install Java 1.8 (OpenJDK 8)\n",
    "brew cask install adoptopenjdk8\n",
    "\n",
    "# Install the current version of Spark\n",
    "brew install apache-spark\n",
    "\n",
    "# Install Py4J (connects PySpark to the Java Virtual Machine)\n",
    "pip install py4j\n",
    "\n",
    "# Add JAVA_HOME to .bash_profile (makes Java 1.8 your default JVM)\n",
    "echo \"export JAVA_HOME=$(/usr/libexec/java_home -v 1.8)\" >> ~/.bash_profile\n",
    "\n",
    "# Add SPARK_HOME to .bash_profile\n",
    "export SPARK_HOME=/usr/local/Cellar/apache-spark/2.4.3/libexec\n",
    "echo \"export SPARK_HOME=/usr/local/Cellar/apache-spark/2.4.3/libexec\" >> ~/.bash_profile\n",
    "\n",
    "# Add PySpark to PYTHONPATH\n",
    "echo \"export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH\" >> ~/.bash_profile\n",
    "\n",
    "# Update current environment\n",
    "source ~/.bash_profile\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:  Confirm installation \n",
    "### Create a SparkSession with a SparkContext\n",
    "\n",
    "- If you have had the this notebook open while running the above commands in the terminal, please close the terminal window, this notebook, and shut down the jupyter server. \n",
    "- Then relauch the jupyter server and reopen this notebook to run the commands below. \n",
    "\n",
    "No matter what, if you cannot get these commands to run in a jupyter notebook before class in the afternoon **do not come to class**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.137.17.88:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x11ee675f8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.137.17.88:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
